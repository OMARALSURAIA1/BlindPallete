{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ccb191f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import zipfile\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdee130",
   "metadata": {},
   "source": [
    "## 1- Restructured Encoder, Decoder, and VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a577b879",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_encoder(latent_dim):\n",
    "    encoder_inputs = layers.Input(shape=(128, 128, 3))\n",
    "    x = layers.Conv2D(32, 3, activation='relu', strides=2, padding='same')(encoder_inputs)\n",
    "    x = layers.Conv2D(64, 3, activation='relu', strides=2, padding='same')(x)\n",
    "    x = layers.Conv2D(128, 3, activation='relu', strides=2, padding='same')(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    z_mean = layers.Dense(latent_dim, name='z_mean')(x)\n",
    "    z_log_var = layers.Dense(latent_dim, name='z_log_var')(x)\n",
    "\n",
    "    def sampling(args):\n",
    "        z_mean, z_log_var = args\n",
    "        epsilon = tf.random.normal(shape=(tf.shape(z_mean)[0], latent_dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "    z = layers.Lambda(sampling, name='z')([z_mean, z_log_var])\n",
    "    return Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "\n",
    "\n",
    "def build_decoder(latent_dim):\n",
    "    decoder_inputs = layers.Input(shape=(latent_dim,))\n",
    "    x = layers.Dense(16 * 16 * 128, activation='relu')(decoder_inputs)\n",
    "    x = layers.Reshape((16, 16, 128))(x)\n",
    "    x = layers.Conv2DTranspose(128, 3, activation='relu', strides=2, padding='same')(x)\n",
    "    x = layers.Conv2DTranspose(64, 3, activation='relu', strides=2, padding='same')(x)\n",
    "    x = layers.Conv2DTranspose(32, 3, activation='relu', strides=2, padding='same')(x)\n",
    "    decoder_outputs = layers.Conv2DTranspose(3, 3, activation='sigmoid', padding='same')(x)\n",
    "    return Model(decoder_inputs, decoder_outputs, name=\"decoder\")\n",
    "\n",
    "\n",
    "class VAE(Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var, z = self.encoder(inputs)\n",
    "        return self.decoder(z)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473d503e",
   "metadata": {},
   "source": [
    "## 2- rebuild the model, dummy input to rebuild the model, and load the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63b794cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load the weights for model\n"
     ]
    }
   ],
   "source": [
    "\n",
    "latent_dim = 32\n",
    "encoder = build_encoder(latent_dim)\n",
    "decoder = build_decoder(latent_dim)\n",
    "vae = VAE(encoder, decoder)\n",
    "\n",
    "\n",
    "dummy_input = tf.random.normal((1, 128, 128, 3))\n",
    "_ = vae(dummy_input)\n",
    "\n",
    "\n",
    "vae.load_weights('vae_final.weights.h5')\n",
    "\n",
    "print(\"Load the weights for model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a0836f",
   "metadata": {},
   "source": [
    "## 3- Prepared paths of train data to preprocess and to git the laten space vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5385fec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_zip(zip_path,extract_folder):\n",
    "  # ÙÙƒ Ø§Ù„Ø¶ØºØ·\n",
    "  with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "      zip_ref.extractall(extract_folder)\n",
    "\n",
    "  print(\"âœ… ØªÙ… ÙÙƒ Ø§Ù„Ø¶ØºØ· Ø¨Ù†Ø¬Ø§Ø­!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75ce130d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ØªÙ… ÙÙƒ Ø§Ù„Ø¶ØºØ· Ø¨Ù†Ø¬Ø§Ø­!\n"
     ]
    }
   ],
   "source": [
    "open_zip('data_split_pants.zip','data_split_pants')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "013db105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ØªÙ… ÙÙƒ Ø§Ù„Ø¶ØºØ· Ø¨Ù†Ø¬Ø§Ø­!\n"
     ]
    }
   ],
   "source": [
    "open_zip('data_split_top.zip','data_split_top')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a583454",
   "metadata": {},
   "outputs": [],
   "source": [
    "pants_train_folder = 'data_split_pants/train'    \n",
    "tops_train_folder = 'data_split_top/train'     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38971526",
   "metadata": {},
   "source": [
    "## 4- Preprocess train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bbd63024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ‘– Pants trained images: (6597, 128, 128, 3)\n",
      "ðŸ‘• Tops trained images: (8159, 128, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "def load_and_preprocess_images(folder_path, target_size=(128, 128)):\n",
    "    images = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(('.jpg', '.png', '.jpeg')):\n",
    "            img_path = os.path.join(folder_path, filename)\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is not None:\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                img = cv2.resize(img, target_size)\n",
    "                img = img.astype('float32') / 255.0  # Normalization\n",
    "                images.append(img)\n",
    "    return np.array(images)\n",
    "\n",
    "\n",
    "pants_train_images = load_and_preprocess_images(pants_train_folder)\n",
    "tops_train_images = load_and_preprocess_images(tops_train_folder)\n",
    "\n",
    "\n",
    "print(f\"ðŸ‘– Pants trained images:\", pants_train_images.shape)\n",
    "print(f\"ðŸ‘• Tops trained images:\", tops_train_images.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfd5ed4",
   "metadata": {},
   "source": [
    "## 5- Merge Tops and Pants Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "faa5cd1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ Merge Data: (14756, 128, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_images = np.concatenate([pants_train_images, tops_train_images], axis=0)\n",
    "\n",
    "# Ø·Ø¨Ø§Ø¹Ø© Ø´ÙƒÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯\n",
    "print(f\"ðŸ“¦ Merge Data:\", train_images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e85c023",
   "metadata": {},
   "source": [
    "## 6- Prepared paths if we want to predict then we print it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d85a435",
   "metadata": {},
   "outputs": [],
   "source": [
    "def git_the_path_in_list(folder,paths):\n",
    "    for filename in sorted(os.listdir(folder)):\n",
    "        if filename.endswith(('.jpg', '.jpeg', '.png')):\n",
    "             paths.append(os.path.join(folder, filename))\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a1ef2e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Tops: 8159\n",
      "Number of Pants: 6597\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tops_paths = []\n",
    "tops_folder = tops_train_folder\n",
    "print(f\"Number of Tops:\", len(git_the_path_in_list(tops_folder,tops_paths)))\n",
    "pants_paths = []\n",
    "pants_folder = pants_train_folder\n",
    "print(f\"Number of Pants:\", len(git_the_path_in_list(pants_folder,pants_paths)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d0ae44",
   "metadata": {},
   "source": [
    "## 7- Prepared the prediected data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f637c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After Preprocessing --> tops_train_images\n",
    "tops_latent_vectors = encoder.predict(tops_train_images)[0]  # Ù†Ø£Ø®Ø° z_mean\n",
    "print(\"Laten Space of Tops:\", tops_latent_vectors.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60f81c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After Preprocessing --> pants_train_images\n",
    "pants_latent_vectors = encoder.predict(pants_train_images)[0]  # Ù†Ø£Ø®Ø° z_mean\n",
    "print(\"Laten Space of Tops:\", pants_latent_vectors.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a296fd",
   "metadata": {},
   "source": [
    "## 8- Find Best Match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "60081b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_matching(img_array_input, latents, paths_of_prediect):\n",
    "    z_mean, _, _ = encoder.predict(np.expand_dims(img_array_input, axis=0))\n",
    "    distances = euclidean_distances(z_mean, latents)\n",
    "    best_index = np.argmin(distances)\n",
    "    return paths_of_prediect[best_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d967d49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_input(path_img):\n",
    "    img = cv2.imread(path_img)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # must change cus this just in colab\n",
    "    img = cv2.resize(img, (128, 128))\n",
    "    img = img.astype('float32') / 255.0\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "99074093",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path ='data_split_top/test/m8.png' \n",
    "process_img = preprocess_input(input_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc24e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_tops_path = find_best_matching(process_img, tops_latent_vectors, tops_paths)\n",
    "print(\"best Top for this pants\", best_tops_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91cf81d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
